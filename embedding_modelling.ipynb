{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# mlp\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_root = \"./data/labels\"\n",
    "\n",
    "# Your local path to the NAIP embeddings data (or a subset/subdirectory of it)\n",
    "# to run on all downloaded data, remove \"wa\" from the path\n",
    "root = \"/mnt/c/data/clay_naip/wa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "true_labels = []\n",
    "false_labels = []\n",
    "for lroot, dirs, files in os.walk(labels_root):\n",
    "    for file in files:\n",
    "        if \"log\" in file:\n",
    "            true_labels.append(gpd.read_file(os.path.join(lroot, file)))\n",
    "        else:\n",
    "            false_labels.append(gpd.read_file(os.path.join(lroot, file)))\n",
    "\n",
    "# concatenate all files\n",
    "true_labels = gpd.GeoDataFrame(pd.concat(true_labels, ignore_index=True))\n",
    "false_labels = gpd.GeoDataFrame(pd.concat(false_labels, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Geographic 2D CRS: EPSG:4326>\n",
       "Name: WGS 84\n",
       "Axis Info [ellipsoidal]:\n",
       "- Lat[north]: Geodetic latitude (degree)\n",
       "- Lon[east]: Geodetic longitude (degree)\n",
       "Area of Use:\n",
       "- name: World.\n",
       "- bounds: (-180.0, -90.0, 180.0, 90.0)\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into single dataframe with label column of 0 and 1 (1 for log yard, 0 for null)\n",
    "true_labels[\"label\"] = 1\n",
    "false_labels[\"label\"] = 0\n",
    "\n",
    "# combine into single dataframe\n",
    "labels = pd.concat([true_labels, false_labels], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3184/3184 [04:52<00:00, 10.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get training data from parquet files\n",
    "\n",
    "def read_parquet_files(root, intersecting_gdf=None, sample_frac=None):\n",
    "    \n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(root):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".parquet\"):\n",
    "                files.append(os.path.join(root, filename))\n",
    "\n",
    "    geoms = []\n",
    "    select_embeddings = []\n",
    "    random_embeddings = []\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "\n",
    "        # read the parquet file\n",
    "        df = gpd.read_parquet(file)\n",
    "        # append the geometry and embedding to the lists\n",
    "        geoms.extend(df.geometry.tolist())\n",
    "        if intersecting_gdf is not None:\n",
    "            # check for intersection with the provided gdf\n",
    "            # use sjoin\n",
    "            df.crs = \"EPSG:4326\"\n",
    "            intersecting_rows = gpd.sjoin(\n",
    "                df, intersecting_gdf, how=\"inner\", predicate=\"intersects\"\n",
    "            )\n",
    "            if len(intersecting_rows) > 0:\n",
    "                select_embeddings.append(intersecting_rows)\n",
    "            \n",
    "        # sample\n",
    "        if sample_frac is not None:\n",
    "            # sample the rows\n",
    "            sampled_rows = df.sample(frac=sample_frac)\n",
    "            # append the sampled rows to the list\n",
    "            random_embeddings.append(sampled_rows) \n",
    "\n",
    "    # return the 2 gdfs\n",
    "    main_gdf = gpd.GeoDataFrame(geometry=geoms, crs=\"EPSG:4326\")\n",
    "    if len(select_embeddings) > 0:\n",
    "        # concatenate the list of dataframes into a single dataframe\n",
    "        select_embeddings = pd.concat(select_embeddings)\n",
    "\n",
    "    if len(random_embeddings) > 0:\n",
    "        # concatenate the list of dataframes into a single dataframe\n",
    "        random_embeddings = pd.concat(random_embeddings)\n",
    "\n",
    "    return main_gdf, select_embeddings, random_embeddings\n",
    "\n",
    "# read the parquet files\n",
    "main_gdf, logyard_embeddings_gdf, random_negative_embeddings = read_parquet_files(root, labels, sample_frac=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the random negative embeddings\n",
    "random_negative_embeddings[\"label\"] = 0\n",
    "\n",
    "# combine with sawmill embeddings\n",
    "logyard_embeddings_gdf = pd.concat([logyard_embeddings_gdf, random_negative_embeddings], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    7340\n",
       "1     972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logyard_embeddings_gdf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to gdf\n",
    "logyard_embeddings_gdf = gpd.GeoDataFrame(logyard_embeddings_gdf, geometry=logyard_embeddings_gdf.geometry)\n",
    "logyard_embeddings_gdf.crs = \"EPSG:4326\"\n",
    "logyard_embeddings_gdf.to_file(\"logyard_embeddings.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = np.array([np.array(x) for x in logyard_embeddings_gdf.embeddings.tolist()])\n",
    "y = logyard_embeddings_gdf.label.values\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      1459\n",
      "           1       0.23      0.22      0.23       204\n",
      "\n",
      "    accuracy                           0.81      1663\n",
      "   macro avg       0.56      0.56      0.56      1663\n",
      "weighted avg       0.81      0.81      0.81      1663\n",
      "\n",
      "MLP:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.90      1459\n",
      "           1       0.24      0.23      0.23       204\n",
      "\n",
      "    accuracy                           0.82      1663\n",
      "   macro avg       0.56      0.56      0.56      1663\n",
      "weighted avg       0.81      0.82      0.81      1663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100), max_iter=1000, random_state=42)\n",
    "\n",
    "# fit the model\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "# make predictions\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "y_pred_mlp = model_mlp.predict(X_test)\n",
    "# print the classification report\n",
    "print(\"RF:\", classification_report(y_test, y_pred_rf))\n",
    "print(\"MLP:\", classification_report(y_test, y_pred_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 1744/3184 [07:13<05:57,  4.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# read the parquet file\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# apply the model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     df \u001b[38;5;241m=\u001b[39m apply_model_to_parquet(file, model_rf, model_mlp, scaler)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/log-embeddings-K-QkMfjf-py3.10/lib/python3.10/site-packages/geopandas/io/arrow.py:775\u001b[0m, in \u001b[0;36m_read_parquet\u001b[0;34m(path, columns, storage_options, bbox, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m     filters \u001b[38;5;241m=\u001b[39m bbox_filter\n\u001b[1;32m    773\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_pandas_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _arrow_to_geopandas(table, geo_metadata)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/log-embeddings-K-QkMfjf-py3.10/lib/python3.10/site-packages/pyarrow/parquet/core.py:1793\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1787\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1788\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated as of pyarrow 15.0.0 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1789\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1793\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/log-embeddings-K-QkMfjf-py3.10/lib/python3.10/site-packages/pyarrow/parquet/core.py:1371\u001b[0m, in \u001b[0;36mParquetDataset.__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partitioning \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1368\u001b[0m     partitioning \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mHivePartitioning\u001b[38;5;241m.\u001b[39mdiscover(\n\u001b[1;32m   1369\u001b[0m         infer_dictionary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/log-embeddings-K-QkMfjf-py3.10/lib/python3.10/site-packages/pyarrow/dataset.py:794\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    783\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    784\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m    785\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes\n\u001b[1;32m    791\u001b[0m )\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, FileInfo) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/log-embeddings-K-QkMfjf-py3.10/lib/python3.10/site-packages/pyarrow/dataset.py:486\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    478\u001b[0m options \u001b[38;5;241m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    479\u001b[0m     partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m    480\u001b[0m     partition_base_dir\u001b[38;5;241m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    481\u001b[0m     exclude_invalid_files\u001b[38;5;241m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    482\u001b[0m     selector_ignore_prefixes\u001b[38;5;241m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    484\u001b[0m factory \u001b[38;5;241m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[38;5;28mformat\u001b[39m, options)\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# apply model per parquet file and collate results\n",
    "def apply_model_to_parquet(file, model_rf, model_mlp, scaler):\n",
    "    # read the parquet file\n",
    "    df = gpd.read_parquet(file)\n",
    "    # get the embeddings\n",
    "    X = np.array([np.array(x) for x in df.embeddings.tolist()])\n",
    "    # scale the data\n",
    "    X = scaler.transform(X)\n",
    "    # make predictions\n",
    "    y_pred_rf = model_rf.predict(X)\n",
    "    y_pred_mlp = model_mlp.predict(X)\n",
    "    # add the predictions to the dataframe\n",
    "    df[\"predictions_rf\"] = y_pred_rf\n",
    "    df[\"predictions_mlp\"] = y_pred_mlp\n",
    "\n",
    "    # drop the embeddings column\n",
    "    df = df.drop(columns=[\"embeddings\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "# List all files in the data directory\n",
    "files = []\n",
    "for root, dirs, filenames in os.walk(root):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".parquet\"):\n",
    "            files.append(os.path.join(root, filename))\n",
    "\n",
    "results = []\n",
    "for file in tqdm(files):\n",
    "    # read the parquet file\n",
    "    df = gpd.read_parquet(file)\n",
    "    # apply the model\n",
    "    df = apply_model_to_parquet(file, model_rf, model_mlp, scaler)\n",
    "    # append to results\n",
    "    results.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all results into a single dataframe\n",
    "results = pd.concat(results, ignore_index=True)\n",
    "# save the results\n",
    "results_file = \"model_predictions.gpkg\"\n",
    "results.to_file(results_file, driver=\"GPKG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-embeddings",
   "language": "python",
   "name": "log-embeddings-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
